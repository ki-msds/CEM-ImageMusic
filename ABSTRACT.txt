ABSTRACTThis study explores the relationship between images and music in the context of computational music generation by asking the question Òwhich set of deep learning-based techniques generate the most appropriate and musical pieces from either colour or emotion representations of image data?Ó The study focuses on and provides a comparative analysis of six image-conditioned music generation systems. These systems were constructed from four models Ð a CNN Image Recognition Model, a K-Means Colour Palette Representation Extraction Model, an LSTM Music Generation Model, and a cGAN Music Generation Model. The outputs of the Image Models serve as conditional variable inputs to the Music Models, through an Russell- Itten Colour/Circumplex Model methodology. A quantitative analysis of the system results show that the most frequent musical instance type was the singular ÒnoteÓ across all systems (18.75%, 26.94%, and 24.38% for cGAN, LSTM, and Stacked system respectively). The average polyphony of the outputs according to system was an average of about 2 notes per musical instance through all musical sequences. It was noted that the cGAN tended to produce a concentrated set of notes, as 16 of the possible 38 musical instance types did not occur in any of the test outputs. The durations across all music model outputs were found to be consistently spread, with all conditional inputs yielding the most common value of 2 and 1Ú4 beats with a frequency of almost 25% through all systems. According to the listening test conducted to analyze the resulting outputs of the constructed systems, the LSTM generated the most musical results regardless of the image model used. 56% of the audio results from the K-Means and 46% of the CNN-driven LSTM outputs generated were considered to be musical. Of the systems compared in this study, the LSTM-based systems yielded the greatest number of responses deemed musical and appropriate, with a total of 109 positive responses against the 54 and 82 of the cGAN and Stacked-based systems respectively. 
It was concluded through the results of the study that focus on construction of such a system should be towards its musicality, the definition of which is the cumulative minimization of Òunwanted soundÓ and the presence of artistic qualities considered Òhuman-like, after which emotional features from an input image may then be used as a meaningful conditioning feature towards generating image-appropriate musical phrases. It was concluded through the combined quantitative system results and qualitative subjective results that listeners found that of the six systems constructed and evaluated, the LSTM-based systems generated the most musical outputs that are also appropriate to the conditioning input image. 